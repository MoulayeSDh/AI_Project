{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34706cd2-8795-49db-bee4-ff2d9c90ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Fine-Tuning DeepSeek-R1 with Unsloth and Hugging Face Dataset\n",
    "# ==========================================================\n",
    "\n",
    "# This script fine-tunes DeepSeek-R1 using Unsloth.\n",
    "# - Loads the \"tatsu-lab/alpaca\" dataset from Hugging Face\n",
    "# - Applies LoRA fine-tuning\n",
    "# - Trains the model efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab5716-ae30-4643-87f8-5bd8c4904b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Step 1: Install Dependencies\n",
    "# ==========================================================\n",
    "!pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "!pip install datasets transformers accelerate\n",
    "\n",
    "# Import necessary libraries\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9962189-4e3b-4d3f-95f7-88203cb0a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Step 2: Load Pre-trained DeepSeek-R1 Model\n",
    "# ==========================================================\n",
    "# Load the model with optimized settings\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
    "    max_seq_length=2048, # Define max sequence length\n",
    "    dtype=None, # Automatically selects optimal dtype\n",
    "    load_in_4bit=True, # Enables 4-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6250e-47c6-4f57-9ba9-d417a039656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Step 3: Apply LoRA for Efficient Fine-Tuning\n",
    "# ==========================================================\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=4, # LoRA rank (4 means lightweight fine-tuning)\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # Layers where LoRA is applied\n",
    "    lora_alpha=16, # Scaling factor for LoRA\n",
    "    lora_dropout=0, # No dropout for stability\n",
    "    bias=\"none\", # No additional bias terms\n",
    "    use_gradient_checkpointing=\"unsloth\", # Enable memory optimization\n",
    "    random_state=42, # Ensures reproducibility\n",
    "    use_rslora=False, # Disable Rank-Structured LoRA\n",
    "    loftq_config=None # No LoFTQ quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48f55c-5c0d-4fef-a6bf-14ec1379160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Step 4: Load and Preprocess the Dataset (Hugging Face)\n",
    "# ==========================================================\n",
    "# Load the \"tatsu-lab/alpaca\" dataset (Instruction-following dataset)\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "\n",
    "# Define a preprocessing function to tokenize inputs\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"Instruction: {q} Response: {a}\" for q, a in zip(examples[\"instruction\"], examples[\"output\"])]\n",
    "    model_inputs = tokenizer(inputs, max_length=2048, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Use a data collator to handle padding dynamically\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3dbd5c-f140-4e2f-a319-fa65cb399d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Step 5: Define Training Arguments\n",
    "# ==========================================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", # Directory to save checkpoints\n",
    "    per_device_train_batch_size=4, # Adjust based on GPU memory\n",
    "    num_train_epochs=3, # Set the number of fine-tuning epochs\n",
    "    logging_dir=\"./logs\", # Logging directory\n",
    "    logging_steps=10, # Log progress every 10 steps\n",
    "    save_total_limit=2, # Keep only the last 2 model checkpoints\n",
    "    save_steps=500, # Save checkpoint every 500 steps\n",
    "    report_to=\"none\", # Disable reporting to WandB or other loggers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d34687-3f3a-4400-a3c1-a64960059345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Step 6: Train the Model\n",
    "# ==========================================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"], # Using the processed dataset\n",
    "    data_collator=data_collator, # Ensure proper padding\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b0c2b-0068-4544-9dbd-24dde2c284e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Step 7: Save the Fine-Tuned Model\n",
    "# ==========================================================\n",
    "model.save_pretrained(\"fine_tuned_deepseek\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_deepseek\")\n",
    "\n",
    "print(\"âœ… Fine-tuned model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

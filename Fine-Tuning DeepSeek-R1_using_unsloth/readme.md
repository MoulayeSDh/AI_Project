# ðŸ¦¾ Fine-Tuning DeepSeek-R1 with Unsloth & Hugging Face Dataset  ðŸ§ 

ðŸš€ **Fine-tune DeepSeek-R1 with LoRA & Unsloth in a memory-efficient way.**  
This repository provides a streamlined **fine-tuning pipeline** for **DeepSeek-R1**, leveraging **Unsloth** and datasets from **Hugging Face**.  
The implementation uses **LoRA (Low-Rank Adaptation)** to efficiently fine-tune the model with minimal computational cost.  

---

## ðŸ”¹ Features  

âœ… **DeepSeek-R1** fine-tuning using **4-bit quantization** for reduced VRAM usage.  
âœ… **LoRA (Low-Rank Adaptation)** - fine-tunes only a subset of model parameters for efficiency.  
âœ… Loads **"tatsu-lab/alpaca"** or any **Hugging Face dataset** for supervised fine-tuning.  
âœ… Optimized training with **PyTorch, Unsloth & Transformers**.  
âœ… **100% Open-Source but strictly Non-Commercial** (see [License](#ðŸ“„-license)).  

---

## ðŸ“‚ Project Structure

Project_AI/ â”‚â”€â”€ fine-tuning DeepSeek-R1_using_unsloth/ â”‚ â”œâ”€â”€ Fine-Tuning DeepSeek-R1-Distill-Llama_using_unsloth.ipynb 

**soon we wil upload full modular version**
---

## ðŸ›  Installation  

### 1ï¸âƒ£ Clone this Repository  

```bash
git clone https://github.com/MoulayeSDh/Project_AI/Project_AI/fine-tuning DeepSeek-R1_using_unsloth.git
cd Project_AI/fine-tuning DeepSeek-R1_using_unsloth/
```
2ï¸âƒ£ Install Dependencies
```
pip install --upgrade --no-cache-dir --no-deps 
git+https://github.com/unslothai/unsloth.git
pip install datasets transformers accelerate torch
```

---

ðŸ— Usage

1ï¸âƒ£ Fine-Tune the Model

Run the following command to fine-tune DeepSeek-R1:
```
python Fine-Tuning DeepSeek-R1-Distill-Llama_using_unsloth.ipynb 
```
By default, the fine-tuned model will be saved in models/fine_tuned_deepseek/.


---

2ï¸âƒ£ Model Inference

After training, you can load and test the fine-tuned model:

from transformers import AutoModelForCausalLM, AutoTokenizer
```
# Load the fine-tuned model
model = AutoModelForCausalLM.from_pretrained("models/fine_tuned_deepseek")
tokenizer = AutoTokenizer.from_pretrained("models/fine_tuned_deepseek")

# Example input
input_text = "What is artificial intelligence?"
inputs = tokenizer(input_text, return_tensors="pt")
output = model.generate(**inputs)

# Print result
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

---

ðŸ“„ License

âš– This project is released under the CC BY-NC 4.0 License.

This means:
âœ… You are free to copy, modify, and distribute this code for non-commercial purposes.
âŒ Commercial use is strictly forbidden without explicit written consent.
ðŸ”— Proper attribution is required when using this project.

> TL;DR: Open-source for research, learning, & personal use. No commercial use allowed.



ðŸ”´ Violating this license will result in a DMCA takedown request.


---

ðŸ™Œ Contributions

Contributions are welcome!

How to Contribute:

1. Fork the repository and create a new branch (feature-improvement or fix-bug).
2. Submit a pull request with clear explanations of changes.
3. Ensure your code follows best practices and passes all tests.
If you find a bug or have a feature request, open an issue.

---

ðŸ‘¨â€ðŸ’» Author

Developed by Moulaye Sidi Dahi
ðŸ’¼ ML Engineer | AI Researcher

ðŸ“§ Email: moulaye.dh@gmail.com
ðŸ”— LinkedIn: Moulaye Sidi Dahi
ðŸ“‚ GitHub: MoulayeSDh


---

ðŸŽ¯ Why This License?

This project is fully open-source but protected from commercial exploitation.

If someone wants to use this work for profit, they must request permission first.

> AI should be accessible to all, not exploited for financial gain.





